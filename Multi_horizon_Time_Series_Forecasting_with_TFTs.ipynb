{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-horizon Time Series Forecasting with TFTs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7TGdfqOOB70bPzllrIxRj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abidshafee/AI-Hub-TTF-Projects/blob/master/Multi_horizon_Time_Series_Forecasting_with_TFTs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzwJS3nvloNb",
        "colab_type": "text"
      },
      "source": [
        "# Temporal Fusion Transformers for Multi-horizon Time Series Forecasting\n",
        "\n",
        "## Introduction\n",
        "This notebook demonstrates the use of the Temporal Fustion Transformer (TFT) for high-peformance mulit-horizon time series prediction, using a traffic forecasting example with data from the UCI PEMS-SF Repository.\n",
        "\n",
        "We also show how to use TFT for two interpretability cases:\n",
        "\n",
        " - Analyzing variable importance weights to identify signficant features for the prediction problem.\n",
        " - Visualizing persistent temporal patterns learnt by the TFT using temporal self-attention weights.\n",
        " \n",
        "A third use case is also presented in our companion notebook \"Temporal Fusion Transfomers for Regime Identification in Time Series Data\".\n",
        "\n",
        "### Reference Paper\n",
        "Bryan Lim, Sercan Arik, Nicolas Loeff and Tomas Pfister. \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\". Submitted, 2019.\n",
        "\n",
        "##### Abstract\n",
        "Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1hJ7ketmqD4",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary Setup\n",
        "### Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjYOQYGMlivH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uses pip3 to install necessary packages\n",
        "!pip3 install pyunpack wget patool plotly cufflinks --user\n",
        "\n",
        "# Resets the IPython kernel to import the installed package.\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ulbThgFmyU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}